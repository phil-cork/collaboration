{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reddit api wrapper\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sqlite3\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Path hack\n",
    "import os\n",
    "# change directory from the current Analysis folder to the top level folder for easier navigation\n",
    "os.chdir('../')\n",
    "# confirm we're at /RedditTextAnalysis\n",
    "print(os.getcwd())\n",
    "\n",
    "# note that the %load_ext autoreload line only needs to be be run once\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by including this autoreload command, we only need to re-import Spatial_Joins if we make/save changes to the original py file\n",
    "%autoreload\n",
    "from Functions.nfl_gamethreads import nfl_gamethreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access password and client secret id via local files\n",
    "with open('pw.txt', 'r') as file1:\n",
    "    pw = file1.read()\n",
    "\n",
    "with open('client_secret.txt', 'r') as file2:\n",
    "    cs = file2.read()\n",
    "\n",
    "# create a praw Reddit instance with app credentials and secret info passed through\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"XbesrQBvKymjgLdgg_D6lA\",\n",
    "    client_secret=cs,\n",
    "    user_agent=\"NFLTextAnalysis/0.0.1\",\n",
    "    username=\"ta_api\",\n",
    "    password=pw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the gamethreads ids from sql\n",
    "gamethreads = pd.read_csv('Data/gamethreads_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"nfl_gamethreads.db\") as conn:\n",
    "\n",
    "    # pull in all submission ids that have previously been crawled\n",
    "    df = pd.read_sql_query(\"SELECT * FROM comments\", conn)\n",
    "    crawled_ids = df['submission_id'].unique().tolist()\n",
    "\n",
    "    all_ids = gamethreads['submission_id'].unique().tolist()\n",
    "\n",
    "    # yields the elements in all_ids not in crawled_ids\n",
    "    remaining_ids = np.setdiff1d(all_ids, crawled_ids)\n",
    "    print(\"Threads remaining: \" + str(len(remaining_ids)))\n",
    "\n",
    "    for each_id in remaining_ids:\n",
    "\n",
    "        comment_df = nfl_gamethreads.get_comments(reddit, each_id)\n",
    "        \n",
    "        comment_df.to_sql(name='comments', con=conn, if_exists='append', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
